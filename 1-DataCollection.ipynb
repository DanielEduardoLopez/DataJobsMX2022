{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tF02PqRjtDyj"
      },
      "source": [
        "# Project: **Data Jobs Salaries in Mexico in July 2022**\n",
        "\n",
        "## 1. Data Collection: Web scraping\n",
        "\n",
        "###### Author: **Daniel Eduardo LÃ³pez**\n",
        "###### GitHub: **_https://github.com/DanielEduardoLopez_**\n",
        "###### Contact: **_daniel-eduardo-lopez@outlook.com_**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The purpose of this notebook is to retrieve job data from the OCC's website (OCC.com.mx) through web scraping. To do so, two functions are defined: \n",
        "- **occscraper**, which scrapes the OCC website and returns the results in a Pandas dataframe, and\n",
        "- **get_classid**, which returns a sample of the OCC website to allow to identify the current class IDs of the OCC website to effectively perform the web scraping.\n",
        "\n",
        "The OCC website dynamically sets the class identifiers for its page elements. So, to perform the web scraping, first, it is strongly advised to first execute the **get_classid** function, then **inspect** what are the **current class identifiers** and finally execute the **occscraper** function to produce the desired results.\n"
      ],
      "metadata": {
        "id": "CR3acpogvndx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3s33xF1ItDyo"
      },
      "outputs": [],
      "source": [
        "# Libraries importation\n",
        "from selenium import webdriver\n",
        "from webdriver_manager.firefox import GeckoDriverManager\n",
        "from selenium.webdriver.firefox.service import Service\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ZbgjfxGtDyr"
      },
      "outputs": [],
      "source": [
        "# Function to scrape job data from OCC.com.mx\n",
        "def occscraper(jobs_list, pages, vacancy_class, jobname_class, salary_class, company_class, location_class):\n",
        "    \"\"\"\n",
        "    This function scrapes job data from the OCC Website (occ.com.mx): Position Name, Salary, Company and Location.\n",
        "\n",
        "    It requires 7 inputs: \n",
        "    1. jobs_list : List with the name of the Data Jobs in both English and Spanish and avoiding empty words (Python list of strings).\n",
        "    2. pages : Number of pages to scrap from the website (Integer).\n",
        "    3. vacancy_class : Class identifier for the vacancy, for instance: 'c0132 c011010' (String)\n",
        "    4. jobname_class : Class identifier for the name of the position, for instance: 'c01584 c01588 c01604 c01990 c011016' (String)\n",
        "    5. salary_class : Class identifier for the salary of the position, for instance: 'c01584 c01591 c01604 c01993' (String)\n",
        "    6. company_class : Class identifier for the company offering the position, for instance: 'c011000' (String)\n",
        "    7. location_class : Class identifier for the geographical location of the position, for instance: 'c011005 c011006' (String)\n",
        "\n",
        "    Output: \n",
        "    1. Pandas Dataframe with the results in a tabular form from the web scraping.\n",
        "\n",
        "    IMPORTANT NOTE: OCC Website dynamically sets the class identifiers for its page elements. So, surely the example class \n",
        "    identifiers will not produce results when running the present code in a different moment than the one when this code was \n",
        "    written and run. Thus, to RE-RUN the code, first, it is strongly advised to first execute the get_classid() function and then \n",
        "    INSPECT what are the CURRENT class identifiers to produce NEW results.\n",
        "\n",
        "    IMPORTANT NOTE 2: This code works with Mozilla Firefox web browser.\n",
        "    \"\"\"\n",
        "\n",
        "    # Setting of the base url of the OCC searcher\n",
        "    base_url = \"https://www.occ.com.mx/empleos/de-\"\n",
        "    base_page_url = \"?page=\"\n",
        "\n",
        "    # Creation of the corresponding url for each job from the jobs list\n",
        "    jobs_url_list = list(jobs_list)\n",
        "    length = len(jobs_url_list)\n",
        "\n",
        "    for i in range(0,length):\n",
        "        jobs_url_list[i] = jobs_url_list[i].strip()\n",
        "        jobs_url_list[i] = jobs_url_list[i].lower()\n",
        "        jobs_url_list[i] = jobs_url_list[i].replace(' ','-')\n",
        "        jobs_url_list[i] = base_url + jobs_url_list[i]\n",
        "        jobs_url_list[i] = jobs_url_list[i] + '/'\n",
        "        #print(jobs_url_list[i])\n",
        "\n",
        "    # Setting of the executable path in a new service instance \n",
        "    service = Service(executable_path=GeckoDriverManager().install())\n",
        "\n",
        "    # Creation of a new instance of the Firefox driver\n",
        "    driver = webdriver.Firefox(service = service)\n",
        "\n",
        "    # Creation of the list to store the data\n",
        "    data = []\n",
        "\n",
        "    # Iterations over the different jobs\n",
        "    for job_url in jobs_url_list:\n",
        "        \n",
        "        # Start of the loop\n",
        "        print('Fetching data for:', jobs_list[jobs_url_list.index(job_url)].title(), \n",
        "            ' ({} out of {})'.format(jobs_url_list.index(job_url)+1, length))\n",
        "        \n",
        "        # Creation of the different pages for the job\n",
        "        pages_url_list = []\n",
        "        for j in range(1, number_pages + 1):\n",
        "            if j == 1:\n",
        "                pages_url_list.append(job_url)\n",
        "            else:\n",
        "                pages_url_list.append(job_url + base_page_url + str(j))\n",
        "            \n",
        "        # Web scrapping over the different pages\n",
        "        for url in pages_url_list:\n",
        "            \n",
        "            # Adding try tag in case urls might have a problem\n",
        "            try:\n",
        "                # Soup creation\n",
        "                driver.get(url)\n",
        "                html = driver.page_source\n",
        "                soup = BeautifulSoup(html, 'html.parser')\n",
        "                \n",
        "                # Data extraction\n",
        "                vacancies = soup.find_all('div', attrs = {'class': vacancy_class})\n",
        "                \n",
        "                for vacancy in vacancies:\n",
        "                    job = []\n",
        "                    \n",
        "                    try:\n",
        "                        job.append(vacancy.find('h2', attrs = {'class': jobname_class}).text)\n",
        "                    except:\n",
        "                        job.append(None) # In case there is no job name available\n",
        "                    \n",
        "                    try:\n",
        "                        job.append(vacancy.find('span', attrs = {'class': salary_class}).text)\n",
        "                    except:\n",
        "                        job.append(None) # In case there is no salary available\n",
        "                    \n",
        "                    try:\n",
        "                        job.append(vacancy.find('a', attrs = {'class': company_class}).text)\n",
        "                    except:\n",
        "                        job.append(None) # In case there is no company name available\n",
        "\n",
        "                    try:\n",
        "                        job.append(vacancy.find('a', attrs = {'class': location_class}).text)\n",
        "                    except:\n",
        "                        job.append(None) # In case there is no location available\n",
        "\n",
        "                    data.append(job)\n",
        "            \n",
        "            except:\n",
        "                continue\n",
        "        \n",
        "        # End of the urls loop\n",
        "        print('Successfully retrieved data for:', jobs_list[jobs_url_list.index(job_url)].title(),\n",
        "            ' ({} out of {})'.format(jobs_url_list.index(job_url) + 1, length) +'\\n')\n",
        "\n",
        "    # End of the main loop\n",
        "    print('Job done!\\n\\n')\n",
        "\n",
        "    # Closure of the Driver\n",
        "    driver.quit()\n",
        "\n",
        "    # Store results as a data frame\n",
        "    df = pd.DataFrame(data, columns = ['Job','Salary','Company','Location'])\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get OCC's current Class IDs\n",
        "def get_classid(jobs_list):\n",
        "    \"\"\"\n",
        "    This function retrieves a sample of the OCC's website to allow the user to identify the current Class IDs for the relevant page elements, in order to allow the\n",
        "    subsequent web scraping of the page.\n",
        "    \n",
        "    It is important to note that OCC Website dynamically sets the class identifiers for its page elements. Thus, to effectively scrape the website, first, it is necessary \n",
        "    to the load the a sample of page source and then INSPECT what are the CURRENT class identifiers to produce results.\n",
        "\n",
        "    IMPORTANT NOTE: This code works with Mozilla Firefox web browser.\n",
        "\n",
        "    Input: \n",
        "    1. jobs_list : List with the name of the Data Jobs in both English and Spanish and avoiding empty words (Python list of strings).\n",
        "\n",
        "    Ouput:\n",
        "    1. Sample of the OCC's website for the first job in the jobs list.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Setting of the executable path in a new service instance \n",
        "    service = Service(executable_path=GeckoDriverManager().install())\n",
        "\n",
        "    # Creation of a new instance of the Firefox driver\n",
        "    driver = webdriver.Firefox(service = service)\n",
        "\n",
        "    # Request of the page source\n",
        "    driver.get(jobs_url_list[0])\n",
        "    html_test = driver.page_source\n",
        "\n",
        "    # Closure of the Driver\n",
        "    driver.quit()\n",
        "\n",
        "    return html_test"
      ],
      "metadata": {
        "id": "Yc4uAVQ8vkKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SY570uSdtDyu"
      },
      "outputs": [],
      "source": [
        "# Entry of the Data Jobs in both English and Spanish (avoid empty words) in a Python list\n",
        "\n",
        "jobs_list = [\"analista datos\",\n",
        "           \"data analyst\",\n",
        "           \"cientifico datos\",\n",
        "           \"data scientist\",\n",
        "           \"ingeniero datos\",\n",
        "           \"data engineer\",\n",
        "           \"arquitecto datos\",\n",
        "           \"data arquitect\",\n",
        "           \"analista negocio\",\n",
        "           \"business analyst\"]\n",
        "\n",
        "# This list was based on: \n",
        "    # Axistalent (2020). The Ecosystem of Data Jobs - Making sense of the Data Job Market. https://www.axistalent.io/blog/the-ecosystem-of-data-jobs-making-sense-of-the-data-job-market \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQdSoogYtDyu"
      },
      "outputs": [],
      "source": [
        "# Number of pages to scrap\n",
        "number_pages = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCDnHQZ9tDyv"
      },
      "outputs": [],
      "source": [
        "# Retrieval of the current class identifiers from the OCC Website\n",
        "get_classid(jobs_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8WLUluytDyw"
      },
      "outputs": [],
      "source": [
        "# Entry of the OCC Website class identifiers\n",
        "vacancy_class = 'c0132 c011010' # Done\n",
        "jobname_class = 'c01584 c01588 c01604 c01990 c011016' # Done\n",
        "salary_class = 'c01584 c01591 c01604 c01993' # Done\n",
        "company_class = 'c011000' # Done\n",
        "location_class = 'c011005 c011006' # Done"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Current time\n",
        "ct = datetime.datetime.now()\n",
        "print(\"current time:\", ct)"
      ],
      "metadata": {
        "id": "i_dgyZNKzt87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMS7dgpktDyx"
      },
      "outputs": [],
      "source": [
        "# Call of the web scraping function\n",
        "df = occscraper(jobs_list, number_pages, vacancy_class, jobname_class, salary_class, company_class, location_class)\n",
        "df "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3bHbZTd2tDyx"
      },
      "outputs": [],
      "source": [
        "# Data exportation as a CSV\n",
        "df.to_csv('DataJobsMexicoJul2022.csv', index=False, encoding='utf-8')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.8.8 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.8"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "6e975f9d2f60b4d9c1da07316fa345238690997a135e24c2e358becc5092ba7f"
      }
    },
    "colab": {
      "name": "1-DataCollection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}